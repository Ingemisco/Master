\section{Implicit Polyline Simplification}\label{sec:implicit_polyline_simplification}
In this section, we review the polyline simplification algorithm and all of its dependencies in order to avoid explicitly computing the solutions of distance equations. We abstract the instances in \citeauthor{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance}'s algorithm for polyline simplification and the required Fréchet distance decision algorithm from \citeauthor{computing_the_frechet_distance_between_two_polygonal_curves} that require explicit solutions to these equations, reducing them to a small set of decision problems.
We show how these decision problems can be solved for the Euclidean distance without using square roots or even divisions, meaning only addition, subtraction, and multiplication are required. The correctness of the implicit algorithms follows directly from the correctness of the explicit version, as only the method of comparison is abstracted while the underlying logic remains unaffected.

\subsection{Related Works}
To our knowledge, all existing algorithms that make use of the Fréchet distance employ the real RAM \cite{computational_geometry_shamos} as their computational model. This model enables storing real numbers and operating on them with addition, subtraction, multiplication, and division in constant time. Furthermore, for the Euclidean distance, it is assumed that square roots can be computed in constant time, and when extending results to arbitrary distance functions, it is assumed that the necessary equations can be solved. \citeauthor{computing_the_frechet_distance_between_two_polygonal_curves}~\cite{computing_the_frechet_distance_between_two_polygonal_curves}, \citeauthor{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance}~\cite{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance}, and \citeauthor{polyline_simplification_has_cubic_complexity_bringmannetal}~\cite{polyline_simplification_has_cubic_complexity_bringmannetal} all present their algorithms in this model, as do many others.

We review the structure of these equation solutions and observe that arbitrary real numbers are not strictly necessary. The implicit approach requires the least powerful computational model in theory by abstracting the equation solving away into the necessary comparisons. Using suitable representations for the involved numbers allows us to assume the regular RAM, at least for the Euclidean, Manhattan, and Chebyshev distances. Whether this suffices for arbitrary Minkowski distances remains an open problem.

\subsection{Decision Problems}
We introduce three decision problems, each based on a relation.
\begin{definition}[Implicit Decision Relations]\label{def:implicit_relations}
  Let \(e = \overline{e_1e_2}\) be a line segment, and let \(u, v\) be points, with \(e_1, e_2, u, v\) all having the same dimension.
  \begin{itemize}
    \item We say \(u\) \emph{reaches} \(e\) (denoted \(u \leftrightarrow e\)) if there exists a point \(x\) on \(e\) with \(\delta(u,x)\leq \varepsilon\). This is equivalent to the existence of \(\hat t_0'(e, u)\).
    \item Let \(u \leftrightarrow e\). We say \(u\) \emph{proceeds to} \(v\) \emph{on} \(e\) (denoted \(u \overset{e}\rightarrow v\)) if and only if
      \begin{itemize}
        \item \(v \leftrightarrow e\),
        \item \(\hat t_0'(e, u) \leq \hat t_0'(e, v)\).
      \end{itemize}

    \item Let \(u \leftrightarrow e\). We say \(u\) \emph{waits for} \(v\) \emph{on} \(e\) (denoted \(u \overset{e}\leftarrow v\)) if and only if
      \begin{itemize}
        \item \(v \leftrightarrow e\),
				\item \(\hat t_0'(e, v) \leq \hat t_0'(e, u) \leq \hat t_1'(e, v)\).
      \end{itemize}
  \end{itemize}
  For \(u \overset e\rightarrow v\), we say that \(v\) becomes the new \emph{restriction point}. Similarly, if \(u \overset e\leftarrow v\), we say that \(u\) remains the restriction point.
\end{definition}

As a note, for \(e = \overline{e_1e_2}\) and a point \(u\), it holds that \(u \leftrightarrow e\) if and only if \(e_1 \overset{e}\rightarrow u\). Thus, it is not necessary to provide a separate implementation for \(\leftrightarrow\). We separate the relations because \(\rightarrow\) is more complex, but often the simpler \(\leftrightarrow\) relation suffices and is geometrically more intuitive.

We will see how we can implement the previous algorithms using procedures to decide these relations for a given line segment and points. The idea is that instead of computing the solutions to the required equations and then comparing them throughout the algorithms, we use procedures to compare them directly. This allows us to only maintain the point that causes the most restrictive solution, which we call the restriction point, as it restricts the earliest reachable point on a line segment. A visual example of these relations can be seen in \cref{fig:restrictions}.

A trivial fact we will use to find initial restrictions is that \(u \overset e\rightarrow v\) always holds if \(v \leftrightarrow e\) and \(u\) is the starting point of \(e\). Also, if both \(u \overset e\rightarrow v\) and \(u \overset e\leftarrow v\), then the first solutions for both are the same, i.e., \(\hat t_0'(e, u) = \hat t_0'(e,v)\); thus, either point can serve as the restriction.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=\linewidth]{tikz-fig/restrictions-1.pdf}
    \caption{\(u \overset e\rightarrow v\). We go from \(\hat t_0'(e, u)\) to \(\hat t_0'(e, v)\), and \(v\) becomes the new restriction point.}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=\linewidth]{tikz-fig/restrictions-2.pdf}
    \caption{Neither \(u \overset e\rightarrow v\) nor \(u \overset e\leftarrow v\).}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=\linewidth]{tikz-fig/restrictions-3.pdf}
    \caption{\(u \overset e\leftarrow v\). We cannot go backward, but \(v\) is reachable, so we can wait for it and \(u\) remains the restriction point.}
  \end{subfigure}
  \caption{Illustration of the relations and how to interpret them.}
  \label{fig:restrictions}
\end{figure}

\subsection{Fréchet Distance Decision}
We review the algorithm from \citeauthor{computing_the_frechet_distance_between_two_polygonal_curves} and adapt it to use the relations from \cref{def:implicit_relations} instead of solving equations explicitly. This can be done both for the general problem of deciding whether the Fréchet distance between two polylines is at most \(\varepsilon\), as well as for the modified version we are interested in, which returns the earliest reachable point on the last line segment\footnote{Obviously, this requires modifications to the statement of the algorithm, as we cannot explicitly compute the solution. These modifications will be mentioned later.}.

For the general Fréchet distance decision problem, fix two given polylines \(P\) of length \(p\) and \(Q\) of length \(q\) with a fixed \(\varepsilon > 0\) and \(p \geq q\) (otherwise, swap the two polylines). We decide if \(\delta^F(P, Q) \leq \varepsilon\) by using the dynamic programming approach from \citeauthor{computing_the_frechet_distance_between_two_polygonal_curves}.

We start by explaining only the version needed where \(q = 1\), meaning the second polyline is a single line segment. As we cannot return an explicit solution to the equation, we return the restriction point \(r\) on the last line segment that determines the solution. Furthermore, the input also takes an initial restriction point \(r'\) for the line segment \(\overline{P(0)P(1)}\) so that we do not have to compute the initial point of the subpolyline.

The start of the algorithm is similar; we first need to test if the initial points match. If so, we can distinguish the two cases where \(p = 1\) or \(p > 1\). Testing if the initial points match is equivalent to testing \(r \overset e\leftarrow Q(0)\) for \(e = \overline{P(0)P(1)}\), as the initial point on \(P\) is \(P(\hat t_0'(e, r))\). The initial points of the polylines have a distance of at most \(\varepsilon\) if this point lies within the interval \([\hat t_0'(e, Q(0)), \hat t_1'(e, Q(0))]\).

\begin{itemize}
  \item[Case \(p = 1\): ] Set \(e = \overline{Q(0)Q(1)}\). The first points of the two line segments already match, so we only need to fully traverse \(Q\). We either need to proceed on \(P\) from the restriction or wait; thus, the result is \(r\) if \(r \overset e\leftarrow Q(1)\), and \(Q(1)\) if \(r \overset e\rightarrow Q(1)\). If neither case occurs, there is no solution.
    See \cref{fig:alt_godau_implicit_eq} for visual examples of this.
  \item[Case \(p > 1\): ] We first traverse the line segment \(e = \overline{Q(0)Q(1)}\) and maintain the current restriction \(r'\), which is initially \(r\). We iterate through \(P(1), P(2), \dots, P(p-1)\), and for each point \(P(i)\), we test if \(r' \overset e\rightarrow P(i)\). If so, we update \(r'\) to \(P(i)\). If \(r' \overset e\leftarrow P(i)\), there is no need to update \(r'\). If neither occurs, there is no solution.

    Finally, after \(P[0\dots p-1]\) is traversed, we determine the restriction on the line segment \(e'=\overline{P(p-1)P(p)}\). The only possible restriction on this line segment is \(Q(1)\), as we need to fully traverse \(e\). If \(Q(1) \leftrightarrow e'\), we can return \(Q(1)\); otherwise, there is no solution.
\end{itemize}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
      \includegraphics[width=\linewidth]{tikz-fig/alt-godau-implicit-eq-1.pdf}
      \caption{\(r \overset e\leftarrow Q(0)\) and \(r \overset e\rightarrow Q(1)\) for \(e = \overline{P(0)P(1)}\). We proceed from the restriction \(r\) to the new restriction \(Q(1).\)}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
      \includegraphics[width=\linewidth]{tikz-fig/alt-godau-implicit-eq-2.pdf}
      \caption{\(r \overset e\rightarrow Q(1)\) but not \(r \overset e\leftarrow Q(0)\) for \(e = \overline{P(0)P(1)}\). Invalid way to proceed on the line.}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
      \includegraphics[width=\linewidth]{tikz-fig/alt-godau-implicit-eq-3.pdf}
      \caption{\(r \overset e\leftarrow Q(0)\) and \(r \overset e\leftarrow Q(1)\), so we can proceed on \(\overline{Q(0)Q(1)}\) while remaining on the same restriction point \(r\) on \(e = \overline{P(0)P(1)}\).}
    \end{subfigure}

    \caption{Modified implicit Fréchet distance, case \(p = 1\).}
    \label{fig:alt_godau_implicit_eq}
\end{figure}

The general algorithm can be adapted with the same ideas, replacing the computations of the first reachable points with the maintenance of the respective restrictions in the dynamic program.

\subsection{Simplification Algorithm}
Now that we have modified the Fréchet distance decision algorithm for the implicit case, adapting the simplification algorithm from \citeauthor{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance} is relatively straightforward.

The dynamic programming table \(DP\) now stores point indices from \(\set{0, \dots, n}\) or a value that indicates that there is no solution for that entry (e.g., \(\infty\)). The point index stored at \(DP[k,i,j]\) is the restriction point \(r\) on the line segment \(\overline{P(j)P(j+1)}\) that would determine the solution in the explicit case.

The initialization remains the same, with a slight abuse of data types. The value \(0\) that indicated the first reachable point on the line segment in the explicit approach now indicates that the point \(P(0)\) is the restriction point. This results in the same solution, as we only store \(0\) for points that have a distance within \(\varepsilon\) from \(P(0)\). Thus, the first solution on any line segment that starts with such a point must be \(0\), which is the same as in the explicit approach.

For \(k > 0\), we only need to adapt how we use the Fréchet distance decision subroutine. During the iteration over \(i\), \(j\), \(i' < i\), and \(j' \leq j\), the explicit approach would retrieve the earliest reachable point \(DP[k-1, i', j']\), which marks the start of the subpolyline we consider. It would then compute the earliest reachable point on the line segment \(\overline{P(j)P(j+1)}\) and return this as a solution. In the implicit version, both of these points are replaced with the respective restriction points that bound them. We have already shown how to adapt the algorithm to take as input the restriction point from \(DP[k-1,i',j']\) and return the new one on the line segment. It remains to show how to minimize over these candidate restriction points.

Throughout the inner iterations over \(i'\) and \(j'\), we maintain the restriction on \(e = \overline{P(j)P(j+1)}\) that yields the earliest reachable point. This is initially \(\infty\). When we find the first valid restriction point, we store it. Once we have a current minimum restriction \(r\) and a new candidate \(r'\), we need to determine whose solution comes first on \(e\). For this, we only need to test \(r \overset e\to r'\). If this holds, \(r\) remains the minimum; otherwise, \(r'\) becomes the new current minimum. Note that in this minimization step, the left side of \(\to\) remains the restriction if it is smaller, whereas in the progression step within the Fréchet decision algorithm, the right side becomes the new restriction.

This concludes the necessary modifications to the algorithm. As a final note, we mention how to apply the optimizations to this approach. The reachability optimization is equivalent to testing \(P(i) \leftrightarrow \overline{P(j)P(j+1)}\). For the minimality conditions, we only need to determine the restriction point that causes the boundary we compared against. A brief analysis shows that we only need to compare the restriction against \(P(i)\) in both cases. \cref{algo:simplify_simple_implicit} shows the full algorithm with the optimizations.

\begin{algorithm}[ht]
  \DontPrintSemicolon
  \KwData{Polyline \(P\) of length \(n\), \(\varepsilon > 0\)}
  \KwResult{Smallest \(\varepsilon\)-simplification of \(P\)}
  \BlankLine
  \(DP \gets Array((n + 1, n + 1, n))\) initialized with \(\infty\) \;
  \For{\(j = 0, \dots, n\)}{
		\If{\(\delta'(P(0), P(j)) > \nu(\varepsilon)\)}{
      \Break
    }
    \(DP[0, 0, j] \gets 0\)
  }
  \For{\(k=1,\dots\) until \(DP[k, n, n-1] \neq \infty\)}{
    \For{\(i=k,\dots, n\)}{
      \For{\(j=0,\dots, n-1\)}{
				\If{\(DP[k-1,i,j] = i\)} {
					\(DP[k,i,j] \gets i\)\;
					\Continue \tcp{Global Minimality}
				}
				\(e \gets \overline{P(j)P(j+1)}\)\;
				\If{\(\lnot (P(i) \leftrightarrow e)\)} {
					\Continue \tcp{Reachability}
				}
        \For{\(k - 1 \leq i' < i\)}{
          \For{\(j' \leq j\)}{
            Let \(r' \gets DP[k-1, i', j']\)\;
						\If{\(r' = \infty\)}{
							\Continue
						}
            Let \(r \gets AltGodauImplicit(P[j' \dots j + 1], r', \overline{P(i')P(i)}, \varepsilon)\)\;
						\If{\(r \neq \infty \land (P(r) \overset e\to P(r'))\)} {
							\(DP[k, i, j] \gets r\)\;
							\If{\(r = i\)} {
								Skip further iterations over \(i', j'\) \tcp{Local Minimality}
							}
						}
          }
        }
      }
    }
  }
  \caption{PolylineSimplificationImplicit(\(P, \varepsilon\))}
  \label{algo:simplify_simple_implicit}
\end{algorithm}


\subsection{Euclidean Implementation}\label{ssec:euclidean-impl}
Having seen how we can use these decision problems to implement the algorithms, we now apply them to the Euclidean case. By avoiding the explicit computation of solutions to \cref{eq:eq_solve_main}, we can avoid computing square roots and even divisions, leading to the following results.

\begin{theorem}
  For \(d\)-dimensional polylines \(P\) and \(Q\) of length \(p\) and \(q\) respectively, it is possible to decide if \(\delta_2^F(P, Q) \leq \varepsilon\) in time \(\O(d p q)\) using only addition, subtraction, and multiplication.
\end{theorem}

\begin{theorem}\label{thm:euclidean_implicit_simplification_simple}
  For a \(d\)-dimensional polyline \(P\) of length \(n\), the problem of global polyline simplification using the Euclidean Fréchet distance can be solved in time \(\O(d k^*n^5)\) using only addition, subtraction, and multiplication, where \(k^*\) is the size of the simplification.
\end{theorem}

Although we have not explained it in full detail, we can improve \cref{thm:euclidean_implicit_simplification_simple} by using the algorithm from \citeauthor{polyline_simplification_has_cubic_complexity_bringmannetal}. We have shown extensively how such a rewrite works, and we provide \cref{tab:explicit-implicit-translation} as a guide for transforming each individual operation to obtain an implicit algorithm.
Furthermore, note that we can also extend this to the Manhattan and Chebyshev distances by using explicit computations but storing the results as numerator-denominator pairs. For this, note that we only compare the fractions but do not perform any further computations with them. Thus, the magnitudes of the numerator and denominator do not explode.

\begin{theorem}\label{thm:euclidean_implicit_simplification_advanced}
  For a \(d\)-dimensional polyline \(P\) of length \(n\), the problem of global polyline simplification using the Euclidean, Chebyshev, or Manhattan Fréchet distance can be solved in time \(\O(d n^3)\) using only addition, subtraction, and multiplication.
\end{theorem}

The decision problems for the relations \(\leftrightarrow, \leftarrow,\) and \(\rightarrow\) are simple to implement without division and square roots, but the implementation is quite technical. We start with \(\leftrightarrow\), which can be implemented as a special case of \(\rightarrow\) but can be solved even more simply. Fix a line segment \(e = \overline{e_1e_2}\) and a point \(u\). The relation \(u \leftrightarrow e\) holds if and only if the distance from \(u\) to the closest point on \(e\) is at most \(\varepsilon\). This closest point with respect to the Euclidean distance is the projection of \(u\) onto \(e\). We write it as \(e_1 + t(e_2 - e_1)\), where \(t = \frac{\braket{e_2 - e_1 | u - e_1}}{\delta_2'(e_1, e_2)}\)~\cite{linear_algebra}.

To decide \(u \leftrightarrow e\), we first check if either endpoint is reachable from \(u\), i.e., \(\delta_2'(e_i, u) \leq \varepsilon^2\) for \(i = 1\) or \(i = 2\). If both fail, we must check the projection. Thus, we return true if and only if \(t \in [0, 1]\) and the following inequality holds:
\begin{alignat*}{3}
&\delta_2'(e_1 + t(e_2 - e_1), u) &&\leq \varepsilon^2 \\
  \iff& \|e_1 - u + t(e_2 - e_1)\|^2 &&\leq \varepsilon^2 \\
  \iff& \delta_2'(e_1, u) + 2\braket{e_1 - u | e_2 - e_1}t + \delta_2'(e_1, e_2)t^2 &&\leq \varepsilon^2.
\end{alignat*}
Substituting \(t = \frac{\braket{e_2 - e_1 | u - e_1}}{\delta_2'(e_1, e_2)}\) and multiplying through by \(\delta_2'(e_1, e_2)\) yields:
\begin{alignat*}{2}
  \iff& \delta_2'(e_1, u)\delta_2'(e_1, e_2) + 2\braket{e_1 - u | e_2 - e_1}\braket{e_2 - e_1 | u - e_1} + \delta_2'(e_1, e_2)\left(\frac{\braket{e_2 - e_1 | u - e_1}^2}{\delta_2'(e_1, e_2)}\right) &&\leq \varepsilon^2 \delta_2'(e_1, e_2) \\
  \iff& \delta_2'(e_1, u)\delta_2'(e_1, e_2) + 2\braket{e_1 - u | e_2 - e_1}\braket{e_2 - e_1 | u - e_1} + \braket{e_2 - e_1 | u - e_1}^2 &&\leq \varepsilon^2 \delta_2'(e_1, e_2).
\end{alignat*}
Noting that \(\braket{e_1 - u | e_2 - e_1} = -\braket{e_2 - e_1 | u - e_1}\), the middle term becomes \(-2\braket{e_2 - e_1 | u - e_1}^2\). Simplifying gives:
\begin{alignat*}{2}
  \iff& \delta_2'(e_1, u)\delta_2'(e_1, e_2) - \braket{e_2 - e_1 | u - e_1}^2 &&\leq \varepsilon^2 \delta_2'(e_1, e_2).
\end{alignat*}
For the other two relations, we get very similar procedures. For simplicity, we only derive the condition for \(u \overset e\rightarrow v\). We define \(a_{0u} \coloneq \delta_2'(e_1, u)\), \(a_{0v} \coloneq \delta_2'(e_1, v)\), \(a_{1u} \coloneq 2\braket{e_2 - e_1 | e_1 - u}\), \(a_{1v} \coloneq 2\braket{e_2 - e_1 | e_1 - v}\), and \(a_{2} \coloneq \delta_2'(e_1, e_2)\), which are the coefficients of the quadratic equations needed to solve for \(\hat t_{0/1}'(e, u)\) and \(\hat t_{0/1}'(e, v)\). We further denote the discriminants of the solutions as \(D_u \coloneq a_{1u}^2 - 4a_{0u}a_2\) and \(D_v \coloneq a_{1v}^2 - 4a_{0v}a_2\). We define \(x  \coloneq a_{1u} - a_{1v}\) and \(y \coloneq D_u + D_v - x^2\). With these, we get:
\begin{alignat*}{2}
  u \overset e\rightarrow v \iff& v \leftrightarrow e \land \hat t_0'(e, u) \leq \hat t_0'(e, v) \\
  \iff& v \leftrightarrow e \land \frac{-a_{1u} - \sqrt{D_u}}{2a_2} \leq \frac{-a_{1v} - \sqrt{D_v}}{2a_2} \\
  \iff& v \leftrightarrow e \land -a_{1u} - \sqrt{D_u} \leq -a_{1v} - \sqrt{D_v} \\
  \iff& v \leftrightarrow e \land \sqrt{D_v} - \sqrt{D_u} \leq a_{1u} - a_{1v}  \\
  \iff& v \leftrightarrow e \land \sqrt{D_v} - \sqrt{D_u} \leq x.
\end{alignat*}
We now analyze the inequality \(\sqrt{D_v} - \sqrt{D_u} \leq x\) by considering the signs of \(x\) and the relative sizes of \(D_u\) and \(D_v\):
\begin{alignat*}{2}
  \iff& v \leftrightarrow e \land \big[ (x \geq 0 \land D_v \leq D_u) \\
  & \quad \lor (x \geq 0 \land D_v \geq D_u \land \sqrt{D_v} - \sqrt{D_u} \leq x ) \\
  & \quad \lor (x \leq 0 \land D_v \leq D_u \land \sqrt{D_u} - \sqrt{D_v} \geq -x ) \big].
\end{alignat*}
Now we consider the inequalities \(\sqrt{D_v} - \sqrt{D_u} \leq x\) (when \(x \geq 0, D_v \geq D_u\)) and \(\sqrt{D_u} - \sqrt{D_v} \geq -x\) (when \(x \leq 0, D_v \leq D_u\)). Since both sides of these inequalities are non-negative, squaring preserves the inequalities. For the first case:
\begin{align*}
  \sqrt{D_v} - \sqrt{D_u} \leq x &\iff D_v + D_u - 2\sqrt{D_uD_v} \leq x^2 \\
   &\iff D_u + D_v - x^2 \leq 2\sqrt{D_uD_v} \\
   &\iff y \leq 2\sqrt{D_uD_v} \\
   &\iff (y \leq 0) \lor (y^2 \leq 4D_uD_v).
\end{align*}
A similar derivation applies to the other inequality. This shows that the problem can be decided without square roots or divisions, albeit in a rather technical manner.

\subsection{General Minkowski Distances}
Having seen how implicit computation simplifies simplification in the Euclidean case, we now explore extending this technique to other Minkowski distances. As of writing, we do not know whether it is possible to implement all three decision problems without approximations for general \(\ell\), but implicit computation may allow a simpler and possibly more numerically stable way to approximate.

First, we consider the relation \(\leftrightarrow\). Again, we can easily check if \(\delta_\ell'(e_1, u) \leq \nu_\ell(\varepsilon)\) and \(\delta_\ell'(e_2, u) \leq \nu_\ell(\varepsilon)\). If neither holds, we need to test if \(\hat t_0'(e, u) \in [0,1]\). This can be solved using Sturm's theorem~\cite{algorithms_in_real_algebraic_geometry}.
\begin{theorem}[Sturm's Theorem]
  Let \(p\) be a polynomial and \(p'\) be its derivative. The number of distinct roots of \(p\) in the interval \((a,b)\) is given by
  \[Var(p_0(a), p_1(a), \dots, p_k(a)) - Var(p_0(b), p_1(b), \dots, p_k(b)),\]
	where \(p_0 \coloneq p\), \(p_1 \coloneq p'\), and \(p_i \coloneq -\text{Rem}(p_{i-2}, p_{i-1})\) for \(i \in \set{2, \dots, k}\) until \(p_{k+1} = 0\).
	Here, \(\text{Rem}\) denotes the remainder of polynomial division, and \(Var\) is the number of sign changes in a sequence, i.e., \(Var(a_0,\dots, a_k) = |\set{i \in \set{1, \dots, k} \mid a_i a_{i-1} < 0}|\).
\end{theorem}

Since our interval is \((0, 1)\), this simplifies the computations, as we only need to evaluate the polynomials at \(0\) and \(1\). Multiplicities of roots do not affect the result, as we are only interested in the existence of a root in the interval.
The only operations needed to implement this are addition, subtraction, multiplication, and division. Since the degree of the polynomials is fixed for a given distance function, we can estimate the required space for a sufficiently precise representation of the coefficients that occur during the polynomial division.

As for the other two relations, it may be possible to use methods from real algebraic geometry to reason about the relative positions of the roots of the polynomials.

As a final note on Minkowski distances, we mention that the absolute values for odd \(\ell\) can be handled similarly to the Manhattan distance by sorting the inner linear terms according to their zeros and partitioning the real line into intervals where all absolute values can be simplified. There are at most two intervals in which the root of the polynomial can lie, and these can be found by evaluating the polynomials at the interval bounds. If any interval has function values of opposite signs at its endpoints, there must be exactly one root in that interval by the intermediate value theorem. This leaves \(\O(d\log d)\) preprocessing to determine a constant number of candidate intervals and thus a constant number of polynomials to test. Alternatively, the linear-time approach from the Manhattan distance can be adapted to find the appropriate interval.

\subsection{Semiexplicit Decisions}
As a final point in our discussion of implicit simplification, we present a method that uses aspects from both explicit and implicit decisions, which we call \emph{semiexplicit}. More specifically, we store some precomputed data as in the explicit case, but as in the implicit case, we avoid computing the solutions explicitly. We present two practical use cases: one for the Euclidean distance and one for general distances.

\subsubsection{General Distances}
The idea is to implement the implicit decisions using an estimation of the roots via a bisection method. To test if \(u \overset e\rightarrow v\), we first check for both \(u\) and \(v\) if they are within \(\varepsilon\) of either endpoint of \(e\). If \(\delta(u, e_1) \leq \varepsilon\), then \(u \overset e\rightarrow v\) is true. If \(\delta(v, e_1) \leq \varepsilon\) then \(\lnot(u \overset e\rightarrow v)\) (but \(v \overset e\rightarrow u\) holds). Otherwise, assume both have solutions in the interior of the segment. 

We perform the following for both \(x \in \set{u, v}\). We start with \(a = 0\), \(b = 1\), and \(t = 0.5\) and iterate until \(d_t \coloneq \delta(e_1 + t(e_2 - e_1), x) \leq \varepsilon\).
First, we test if \(d_t \leq \varepsilon\); if so, we are done with this step. Otherwise, we examine the behavior of the distance function. Due to the convexity property (\cref{lem:distance_properties}), the distance function is unimodal along the line segment. We compute \(d_a\) and \(d_b\). If \(d_a \leq d_t \leq d_b\), we proceed with \(b \gets t\) and \(t \gets (a + t)/2\). If \(d_a \geq d_t \geq d_b\), we proceed with \(a \gets t\) and \(t \gets (t + b)/2\). If neither pattern holds, the function must have a minimum between \(a\) and \(b\) (i.e., \(d_t\) is less than both \(d_a\) and \(d_b\)). We then compute \(i \coloneq (a + t)/2\) and \(j \coloneq (t + b)/2\). It holds that \(a < i < t < j < b\). We then check the distances at \(i\) and \(j\) to refine the interval containing the minimum. This process narrows the interval believed to contain the first solution \(\hat t_0'(e, x)\).

After this process for both \(u\) and \(v\), we have intervals \((a_u, t_u]\) and \((a_v, t_v]\) that contain their respective solutions. If these intervals are disjoint, we can immediately determine which solution comes first. Otherwise, we restrict both intervals to their intersection and perform a bisection within this shared interval, comparing the distances for \(u\) and \(v\) simultaneously until we find a point that separates their first solutions. There are edge cases to consider, such as when one point has no solution within the interval. While Sturm's theorem could be used to test for solutions exactly, it might be slower than continuing the bisection to a sufficient precision.

The semiexplicit approach may seem worse than both the explicit approach (as it requires multiple comparisons) and the implicit approach (as it relies on approximating solutions). However, its advantage is generality. The explicit approach requires a distance function and an equation solver. The implicit approach requires the distance function and solvers for the decision problems. The semiexplicit approach only requires the distance function as a black box, with no further information; thus, it also works for general\footnote{The distance function must still be convex in the sense of \cref{lem:distance_properties}.} distance functions on \(\R^d\), not only Minkowski distances. Furthermore, if the first solutions for the two points are easily separable, the approximations can speed up computations. In the worst case, the solutions are close or coincide, requiring many iterations; however, this is unlikely and may not critically affect the overall simplification\footnote{It is possible to construct egenerate polylines where the solutions are arbitrarily close and the result matters.}.

It can be optimized by storing the resulting intervals in the dynamic programming table as a substitute for the earliest reachable point. The respective restriction point is still required because the interval may need refinement when compared against another. This allows fewer total computations while retaining the advantage of lazy computation: solutions are only approximated to the precision necessary for the decision, which can be fast for well-behaved polylines.

\subsubsection{Euclidean Distance}
In the Euclidean case, we have seen that the implicit algorithm primarily involves comparisons of the form \(a - \sqrt{b} \leq c \pm \sqrt{d}\), which is similar to the explicit case but with canceled denominators.

In the pure implicit approach, we recompute the values \(a, b, c, d\) for each comparison. To reduce computations, we can modify the explicit algorithms to store pairs of the form \((a, b)\) representing the value \(a - \sqrt{b}\) (ignoring the common denominator) and define comparison operators on these pairs directly.

We have already shown how to solve these comparison decisions in \cref{ssec:euclidean-impl}. In programming languages that allow operator overloading (e.g., for \(\leq\)), the resulting semiexplicit version and the explicit version differ only in the datatype of the stored solutions, making them parametrically polymorphic variants of the same algorithm.

Note that solutions are always of the form \(a - \sqrt{b}\); the form \(a + \sqrt{b}\) is only used to define the interval \([a - \sqrt{b}, a + \sqrt{b}]\) (again, ignoring the common denominator).

This approach for the Euclidean distance translates more easily to other simplification algorithms than the pure implicit approach but maintains the same advantages. It acts as an implicit version that caches as much of the computation as possible.

\subsection{Conclusion}
We have seen how to rewrite algorithms into their implicit and semiexplicit versions. \cref{tab:explicit-implicit-translation} summarizes how to replace each operation in an explicit algorithm to obtain an implicit version.

\begin{table}[htb]
  \centering
	\begin{tabular}{|lll|}
		\hline
		Explicit operation & Implicit operation & Comment\\
		\hline
		\(t \gets \hat t_0'(e, u)\) & \(r \gets u\) & The restriction \(r\) replaces the solution \(t\)\\
		\(t_0 \leq t_1\) & \(r_0 \overset e\rightarrow r_1\) & \(t_0, t_1\) are solutions on \(e\), \\
		& & and \(r_0, r_1\) are the respective restrictions\\
		\(t \gets \min(t_0, t_1)\) & if \(r_0 \overset{e}\rightarrow r_1\) then \(r \gets r_0\) else \(r \gets r_1\) & \(t, t_0, t_1\) are solutions on \(e\) \\
		& & and \(r, r_0, r_1\) are the respective restrictions\\
		\(t \gets \max(t_0, t_1)\) & if \(r_0 \overset{e}\rightarrow r_1\) then \(r \gets r_1\) else \(r \gets r_0\) & \(t, t_0, t_1\) are solutions on \(e\) \\
		& & and \(r, r_0, r_1\) are the respective restrictions\\
		\(\hat t_0'(e, u) \leq t \leq \hat t_1'(e, u)\) & \(r \overset e\leftarrow u\) & \(r\) is the restriction for \(t\)\\
		\(\hat t_0'(e, u)\) exists & \(u \leftrightarrow e\) & \\
		\hline
	\end{tabular}
	\caption{Explicit-to-Implicit Translation Table}
	\label{tab:explicit-implicit-translation}
\end{table}

\begin{enumerate}
	\item We conclude this discussion of the implicit approach with the following open question: Can the decision problems be solved exactly for general \(\ell\)-Minkowski distances with \(\ell \in \N\) in the regular RAM model? 

	We have proposed a method to decide the \(\leftrightarrow\) relation using Sturm's theorem but have not fully solved the other two relations. This likely requires leveraging more specific properties of the distance polynomials, such as that they have at most two roots or are sums of linear terms raised to the power \(\ell\). These results would likely be of theoretical interest, simplifying the required computational model but not necessarily leading to practical algorithms.

	\item Are good semiexplicit representations for \(\ell\)-Minkowski distances other than the Euclidean distance?

	For \(\delta_3\) and \(\delta_4\) it might be possible to create such a representation from the respective solution formulas. Since these formulas are very long and complicated, creating such a representation would be tedious if it is even possible.
\end{enumerate}

