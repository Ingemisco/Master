\section{Adapting Imai and Iri to the Global Fréchet Distance}
\label{sec:global_imai_iri}

In this section, we revisit the well-known \citeauthor{computational_geometric_methods_for_polygonal_approximations_of_a_curve}~\cite{computational_geometric_methods_for_polygonal_approximations_of_a_curve} algorithm for local Fréchet polyline simplification and demonstrate how to adapt it to the global Fréchet setting. In doing so, we rediscover the simplification algorithm from \citeauthor{global_curve_simplification}. This presents a new perspective on their algorithm and the link to the \citeauthor{computational_geometric_methods_for_polygonal_approximations_of_a_curve} algorithm might allow to adapt optimizations from the local setting.

Throughout this section we fix \(\varepsilon > 0\) and a distance \(\delta\). Further, let \(P\) be a polyline of length \(n\). We note that some of the lemmas in this section can be seen rather trivial in the free space diagram. We have not mentioned it in this thesis because it adds a layer of abstraction which makes the algorithms less intuitive in our opinion. Furtermore, up until this section there was no need for it. An introduction to it is longer than proving the lemmas directly.

\subsection{Local Case}

We recall the algorithm in the local setting to identify where modifications are needed to adapt it to the global setting. 

The algorithm consists of two main parts: first, a shortcut graph is constructed and then, a shortest path computation yields the simplification.

The shortcut graph is a directed acyclic graph whose vertices correspond to the polyline vertices. A directed edge \((P(i), P(j))\) with \(i < j\) exists if \(\delta^F(P[i \dots j], \overline{P(i)P(j)}) \leq \varepsilon\) (i.e., \(\overline{P(i)P(j)}\) is a shortcut for the subpolyline \(P[i \dots j]\)). 

To determine whether \(\delta^F(P[i \dots j], \overline{P(i)P(j)}) \leq \varepsilon\), we use the algorithm from \citeauthor{computing_the_frechet_distance_between_two_polygonal_curves}, modified versions of which are outlined in \cref{ssec:alt_godau,ssec:implicit_frechet_decision}. Here, the simpler classical version suffices. This test requires \(\Oh(n)\) time and as there are \(\O(n^2)\) many pairs to test, constructing the shortcut graph requires \(\Oh(n^3)\) time.

Each path from \(P(0)\) to \(P(n)\) corresponds to a valid, not necessarily optimal, local simplification of the polyline by construction and it is obvious that all such simplifications are captured in the shortcut graph. Thus, computing the shorted \(P(0)-P(n)\) path in this graph yields an optimal local simplification. This can be done easily in \(\O(n^2)\).

The total runtime is determined by the cubic graph construction phase thus the total runtime is \(\Oh(n^3)\).

\subsection{The Global Shortcut Graph}
In the global case the structure of these shortcuts is more complicated, necessitating a more sophisticated approach. To confront this problem, we store for each shortcut the set of all subpolylines for which it is a valid shortcut. 

\begin{definition}
  For a shortcut \(e = \overline{P(i')P(i)}\) we say that \((t', t) \in [0, n]^2\) with \(t' \leq t\) is \emph{\(e\)-admissible} if and only if \(\delta^F(P[t' \dots t], e) \leq \varepsilon\). We denote \(\mathcal{A}_e = \set{(t', t) \mid (t', t) \textrm{ is } e\textrm{-admissible}}\) as the set of \(e\)-admissible subpolylines.
\end{definition}

The following observation might seem complicated, but it follows directly from the definitions of admissibility and Fréchet distance. It captures our intuition that global simplifications are constructed of only admissible subpolylines.

\begin{observation}
	Let \(Q=\angl{P(u_0), \dots, P(u_q)}\) be a (not nessarily optimal) global simplification of \(P\) which means there are parameterizations increasing, continuous \(f, g\) with \(f([0, 1]) = [0, n]\) and \(g([0,1]) = [0, q]\) with \(\delta(P(f(\alpha)), Q(g(\alpha))) \leq \varepsilon\) for all \(\alpha \in [0, 1]\).

	Then, for \(i \in \set{1, \dots, q}\) and \(x \leq y \in [0, 1]\) such that \(g(x) = i - 1\) and \(g(y) = i\) it holds that 
	\[\delta^F(P[f(x) \dots f(y)], \overline{P(g(x)), P(g(y))}) \leq \varepsilon.\]
	This means \((f(x), f(y))\) is \(e\)-admissible for \(e = \overline{P(g(x), P(g(y)))} = \overline{Q(i-1), Q(i)}\).
\end{observation}

Next, we explore the set of admissible subpolylines \(\mathcal{A}_e\) with the goal of establishing an efficient representation.

\begin{lemma}\label{lem:admissible_are_intervals}
	Let \(e = \overline{P(i')P(i)}\).
  \begin{enumerate}
		\item Let \((r, t)\in \mathcal{A}_e\) and \(r \leq t' \leq t\) with \(\delta(P(t'), P(i')) \leq \varepsilon\). Then \((t', t) \in \mathcal{A}_e\).
		\item Let \((t', r)\in \mathcal{A}_e\) and \(t' \leq t \leq r\) with \(\delta(P(t), P(i)) \leq \varepsilon\). Then \((t', t) \in \mathcal{A}_e\).
  \end{enumerate}
\end{lemma}

\begin{proof}
	We show the first statement. The second one is analogous. We create suitable parameterizations \(f\) and \(g\) to show \(\delta^F(P[t' \dots t], e) \leq \varepsilon\). As \((r,t)\) is \(e\)-admissible, there are functions increasing, bijective parameterizations \(f':[0,1] \to [r \dots t]\) and \(g':[0,1] \to [0,1]\) (here we shift the range of \(f'\) to be consistent with \(P\) and not the subpolyline \(P[r \dots t]\)).

	Since \(f'\) is surjective onto \([r, t]\) and \(t' \in [r, t]\), there is \(x \in [0, 1]\) with \(f'(x) = t'\). This means that \(\delta(P(t'), e(g'(x))) \leq \varepsilon\). By the convexity property in \cref{lem:distance_properties}, \(\delta(P(t'), e(\lambda)) \leq \varepsilon\) for all \(\lambda \in [0, g'(x)]\). With this, we construct the parameterizations \(f\) and \(g\) such that \(f\) is the zero function until \(g\) assumes the value \(g'(x)\). After this point both functions are identical to \(f'\) and \(g'\) respectively (up to a shift in the range).
\end{proof}

\begin{definition}
	For sets \(A\) and \(B\) we define \(A \times_{\leq} B\) to be 
		\[A \times_{\leq} B = \set{(a, b) \in A \times B \mid a \leq b}.\]
\end{definition}

Using \cref{lem:admissible_are_intervals}, we define an efficient representation of \(\mathcal{A}_e\).
\begin{lemma}\label{lem:admissible-rep-1}
	Let \(e = \overline{P(i')P(i)}\) be a line segment. Denote \(\mathcal{I} = \set{t \in [0,n] \mid \delta(P(i'), P(t)) \leq \varepsilon}\) and \(\mathcal{J} = \set{t \in [0,n] \mid \delta(P(i), P(t)) \leq \varepsilon}\). There exist \(k \leq n\) and families of closed intervals \(I_1 \leq \cdots \leq I_k\) and closed intervals \(J_1 \leq \cdots \leq J_k\) such that 
	\[\mathcal{A}_e = \parenth{\mathcal{I} \times_\leq \mathcal{J}} \cap \bigcup_{j=1}^k I_j \times J_j.\]

	Here, the ordering on the intervals is defined by their lower bounds or equivalently by their upper bounds. It is further possible to achieve that the family \(I_j\) is pairwise disjoint or that the family \(J_j\) is pairwise disjoint (but not necessarily both).
\end{lemma}

\begin{proof}
	The lemma is trivial if \(\mathcal{A}_e\) is empty. Suppose it is not empty. It holds by definition that \(\mathcal{A}_e \subseteq \mathcal{I} \times_\leq \mathcal{J}\). 

	For any \(e\)-admissible subypolline \((t', t)\) there exist closed maximal intervals \(I\) and \(J\) with \(t' \in I\), \(t' \in J\), and \(I \subseteq \mathcal{I}\) and \(J \subseteq \mathcal{J}\) by the distance constraints on the endpoints by the definition of the Fréchet distance. 

	We show that \(I \times_\leq J \subseteq \mathcal{A}_e\): pick any \(s' \leq s\) with \(s' \in I\) and \(s \in J\). If \(s' \leq t'\) then \(\delta^F(P[s' \dots t'], P[y \dots y]) \leq \varepsilon\) for any \(y \in J\) by construction of \(I\) and \(J\), and thus, we can extend any admissible \(t', y\) subpolyline to an admissible \((s', y)\). Similarly, if \(s \geq t\), for any \(x \in I\), we can extend admissible \((x, t)\) subpolylines to admissible \((x, s)\) subpolylines. Therefore, we can assume \(s' \geq t'\) and \(t \leq s\). Under these conditions, it follows that \((s', s)\) is admissible by using \cref{lem:admissible_are_intervals}.

	We now have seen that admissibility is a property shared by pairs of intervals, not only points. Thus, we will say \((I, J)\) is admissible if all point pairs \((t', t)\) with \(t' \leq t\) in \(I \times J\) are admissible.


	This shows that \(\mathcal{A}_e\) has almost such a structure. We need to show that \(k \leq n\) many interval pairs suffice. For this, we merge intervals: Let \(I < I'\) disjoint and \(J\) be closed intervals that we have already selected for the construction. Furthermore, assume that \((I, J)\) and \((I', J)\) are admissible. By \cref{lem:admissible_are_intervals}, for any point \(x \in [\min I, \max I'] \cap \mathcal{I}\) any \(y \in J\) with \(x \leq y\) must form an admissibily subpolyline \((x, y)\). Thus we merge all such intervals \(I'\) and \(I\) to form an interval of intervals per interval \(J\). This does not introduce no inadmissible subpolylines, as we already filter by \(\mathcal{I} \times_\leq \mathcal{J}\). As each interval \(J\) has a unique range of intervals \(I\), the \(J_j\) are disjoint. By merging the \(J\) per \(I\) instead, we can achieve disjointness of \(I_j\).

	The total number of interval pairs is bounded by the number of possible closed intervals that make up \(\mathcal{I}\) and \(\mathcal{J}\). There can be at most one closed interval per line segment of the polyline, thus \(k \leq n\), concluding the proof.
\end{proof}


\begin{definition}[Global Shortcut Graph]
	For a polyline \(P = \angl{v_0, \dots, v_n}\) and \(\varepsilon > 0\) we define the global shortcut graph as the directed acyclic graph \(G = (V, E)\) with \(V = \set{v_0, \dots, v_n}\) and \(E = \set{(v_i, v_j) \mid i < j, e = \overline{v_iv_j}, \mathcal{A}_e \neq \emptyset}\).

	We define a labelling function on the edges 
	\[S(e) = ((I_1, J_1), (I_2, J_2), \dots, (I_k, J_k)),\]
	where the intervals \(I_i, J_i\) are taken from the proof of \cref{lem:admissible-rep-1}. We extend this labelling on the vertices by \(S(v) = (I_1 < \cdots < I_k)\), which are the intervals that make up \(\mathcal{I}\) from the lemma.
\end{definition}

Let us briefly analyze the space consumption of the global shortcut graph: for each line segment, we may need to store up to \(n\) interval pairs, which in total requires cubic space. There are at most \(n\) solution intervals in \(\mathcal{I}\) per point, thus requiring total quadratic space. Thus, the total information of the global shortcut graph requires \(\O(n^3)\) space. This, however is only the worst case. Since intervals can span multiple line segments and because we merge them there may be fewer intervals. For further analysis, we denote \(\mathcal{E}\) the total number intervals used for the edges and \(\mathcal{V}\) for the number of intervals for the vertices. The total space thus is \(\O(n^2 + \mathcal{E})\).

We finally consider the runtime of constructing the global shortcut graph. For the point \(u\), the intervals \(S(e)\) can be determined by solving the necessary equation for each line segment of \(P\) and potentially merging ones that span multiple line segments. This takes runtime \(\Oh(n)\) per point and thus in total \(\Oh(n^2)\). 

To determine the intervals in \(S(e)\) for the line segment \(e = \overline{P(i')P(i)}\), we use a modified version of \citeauthor{polyline_simplification_has_cubic_complexity_bringmannetal}'s cell reachability algorithm that we have outlined in \cref{ssec:cell_reachability}.

\subsection{Modified Cell Reachability}
We outline how to modify the cell reachability problem to construct the intervals in \(S(e)\). We are given the ordered lists of intervals \(S(i')\) and \(S(i)\) and want to find the merged intervals. 

The setting is rather similar with few differences. First, there are no entry- or exit-costs, we only track admissibility. Second, the result is an interval \(J_j\) for each admissible interval \(I_j\). The passages between cells are the solution intervals of the \(e\) and the respective point. For an example instance refer to \cref{fig:ex_cr_globalii}.

Only few modifications are needed: For each passge, we remove all queue entries at the front with a value below the lower bower bound of the passage and reinsert them with them as a single entry whose value is the lower bound. Each entry is now associated with an interval of indices which correspond to the intervals in \(S(i')\). When merging, we update the upper bound of the indices interval. If an interval in \(S(i')\) starts before the cell passage, it must be accounted for during the merging step.

Then, we remove all entries that lie above the upper end of the passage. Each of them contains an indices interval that represent  the intervals for which the last seen \(S(j')\) interval is the upper end. For all of these intervals we update their upper bound.

The intervals in \(S(i)\) can be ignored, if we are only interested to compute maximal intervals \(J_j\) for each \(I_j\). We can consider them to only store discrete intervals that represent the intervals in \(S(i)\) that are to be used.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1, width=0.9\linewidth]{./tikz-fig/ex_cr_globalii.pdf}
	\caption{Cell reachability example for computing \(S(\overline{P(0)P(1)})\). Here, \(S(0) = ([0, 0.5], [2.5, 2.5]), [3.59, 4.36]\) are marked in blue, and \(S(1) = ([0.5, 1.70], [2.5, 2.5], [4.64, 5])\) are marked in red. The one point interval \([2.5,2.5]\) is shared by both and is marked in purple. It is not a vertex of the polyline. We can see that \(S(\overline{P(0)P(1)}) = (([0,0.5], [0.5,5]), ([2.5,2.5],[2.5,5]), ([3.59,4.36],[4.64,5]))\).}
  \label{fig:ex_cr_globalii}
\end{figure}

A small optimization that can be applied is to skip cells when the queue becomes empty and resume at the next \(S(i')\) interval. Using this, the total runtime per line shortcut is proportional to the total length of the intervals which extends to the construction of the shortcut graph.

\subsection{Simplification Construction}

To construct the simplifications from the graph, we iterate through it using the admissible subpolylines. Any simplification is a sequence of \(e_i\)-admissible subpolylines where the sequence of edges \(e_i\) form a 0-n-path and the endpoints of two consecutive subpolylines match.

Unlike the local case, it does not suffice to find a shortest path, thus we have to consider more wisely how to iterate through the graph. Interestingly, we can derive the algorithm from \citeauthor{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance} using suitable graph traversels: we start with \(S(0)\) as the starting interval and traverse the whole graph to find all intervals to which we can procede. In the subsequent traversals we repeat this procedure but start with the computed intervals from the last step. We stop once an interval is found that contains the endpoint of the polyline.

From this, we can see why the \citeauthor{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance} algorithm has poor runtime: we iterate through the whole graph multiple times. Furthermore, their algorithm does not precompute the global shortcut graph but computes everything once it is used which also means that the same computations are performed multiple times. Using our precomputed graph the runtime can be reanalyzed as follows: let the simplification size be \(k\), then the total runtime is \(\Oh(n^3 + kf(n))\) where \(f(n)\) is the runtime per iteration and \(\Oh(n^3)\) the runtime to construct the graph.

Let us analyze how many intervals there can be at most that need to be tracked: each interval is paired with an vertex from the polyline where the vertex represents the last point of the partial simplification (i.e., the point form which we start the next shortcut) and the interval is the end of the subpolyline. As seen previous, there can be at most one interval per line segment but as it can be paired with different points, there can be in total \(\O(n^2)\) many intervals per iteration to track. For each interval we must iterate over all possible shortcuts that start from its associated end vertex to compute the next interval and verex pairs. There are at most \(\O(n^2)\) many possibilities to iterate through, resulting in \(\O(n^4)\) runtime per traversal and thus in total \(\Oh(kn^4)\) runtime. This improves upon the standard algorithm by \(\O(n)\). Furthermore, this version of the algorithm benefits more from the well-behavedness of the input by having fewer intervals to track.

To obtain a better runtime, we traverse the graph only once and track all intervals. For this, each interval needs to track not only the end vertex but also the length of the partial simplification. Similarly to the algorithm by \citeauthor{polyline_simplification_has_cubic_complexity_bringmannetal}, we iterate through the polyline vertices. Doing so, we need to track not only the reachable ranges and the end vertex of the polyline from which the simplification must continue, but also we need to track the size of the partial simplification. This size was previously implicitly tracked as the size corresponds to the iteration in the algorithm by \citeauthor{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance}. Na"ively, this does not improve the algorithm as there can be a cubic number of triples to track instead of only quadratically which results in the same runtime. However, we still only need to store a quadratic number of triples still, since we actually only need the same data as previously. This is because we never need to store different triples that share the range and end vertex but only differ in size as we minimize for the size and it is trivial to see that a triple with the same interval and end vertex but higher size can never be used in an optimal simplification. Thus, the total amount of data per vertex is only in \(\O(n^2)\). For each of those we iterate over the respective \(s(e)\) to compute the next ones in total cubic runtime. Thus, the total algorithm has \(\Oh(n^4)\) runtime. This is identical to the \(\Oh(n^4)\) algorithm by \citeauthor{global_curve_simplification}

\subsection{Cubic Runtime}
Similarly to \citeauthor{polyline_simplification_has_cubic_complexity_bringmannetal}, \citeauthor{global_curve_simplification} present a cubic runtime algorithm based on the same idea of differentiating two cases that behave differently. 






\subsection{Subcubic Runtime}
Finally, we want to reanalyze the runtime more closely to investigate under which conditions this algorithm can achieve subcubic runtime. We have already seen, that the runtime to construct the global shortcut graph can be subcubic if the total amount of points that need to be considered in the intervals is small. Let us first formalize this idea.







