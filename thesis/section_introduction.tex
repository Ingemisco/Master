\section{Introduction}
\label{sec:introduction}

Polyline simplification is the process of algorithmically reducing the number of points of a polyline while preserving its general shape up to a specified error parameter \(\varepsilon\). This technique is widely used in applications such as geographic information systems (GIS) to simplify map contours~\cite{algorithms_reduction_number_points_caricature}. To evaluate the quality of a simplification, distance functions between polylines are employed, with the Hausdorff and Fréchet distances being the most common. These distances can be applied either \emph{locally}, where each segment of the simplified polyline is compared with the corresponding subpolyline of the original, or \emph{globally}, where the entire simplification is compared to the original as a whole.

Local simplification has been extensively studied for both the Hausdorff and the Fréchet distances~\cite{polyline_simplification_under_the_local_frechet_distance_has_almost_quadratic_runtime_in_2d_storandtetal,computational_geometric_methods_for_polygonal_approximations_of_a_curve}. In contrast, global simplification has only recently gained attention. \citeauthor{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance} proposed a polynomial-time algorithm for global Fréchet simplification and proved that global Hausdorff simplification is NP-hard. Later, \citeauthor{polyline_simplification_has_cubic_complexity_bringmannetal} improved this result by developing a cubic-time algorithm and establishing conditional lower bounds for global Fréchet simplification as well as both local variants. 

In this thesis, we discuss these two algorithms in detail with examples. We explain every step down from solving necessary equations to the full simplification algorithms effectively making this thesis a full manual for simplification algorithm implementation. Further, we present a wide array of optimizations to achieve better practical runtimes for these algorithms such as parallelization, compiler configuration, and hardware-related considerations. 

First, we define terminology, definitions, and mathematical concepts used throughout this thesis in \cref{sec:preliminaries}. Then, in \cref{sec:preliminaries} we dsitinguish the more commonly used local variants from the global variants in which we are interested in and derive simple properties to get an intuition on how to work with the Fréchet distance. 

Before we discuss simplification algorithms, we show how to solve various necessary equations in \cref{sec:equation_solving} where we derive the formula for the Euclidean case and algorithms for the Manhattan and Chebyshev cases. Following this discussion, we explain the algorithm from \citeauthor{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance} for polyline simplification as well as the required algorithm from \citeauthor{computing_the_frechet_distance_between_two_polygonal_curves}. We visualize the algorithm along examples to further intuition and motivate optimizations that can be applied. 

we continue more theoretically by proposing our implicit Fréchet framework in \cref{sec:implicit_polyline_simplification} and show how to transform current approaches, which we call explicit, into implicit ones along the examples of the prviously discussed algorithms. The implicit approach relies on the fact that we never need to perform arithmetic on the solutions of equations from \cref{sec:equation_solving} but only comparisons. As a consequence, we can show that in the Euclidean case polyline simplification can actually be solved without the use of square root and division operations in the same (theoretical) runtime. For general distances, the implicit approach is applicable and can possibly avoid the need of finding roots of higher-order polynomials but the implicit variants need to solve problems in real algebraic geometry. 

\cref{sec:cubic_algo} explains the algorithm from \citeauthor{polyline_simplification_has_cubic_complexity_bringmannetal} and the required cell reachability problem. Again, we explain the algorithms utilizing examples and mention practical considerations.

Having seen two algorithms to simplify a polyline for a single \(\varepsilon\), we present an algorithm that computes \emph{all} simplifications for a polyline in \cref{sec:simplification-queries}. For this we determine the intervals in which the simplification size does not change. This allows the creation of a datastructure that facilitates querying for simplifcations given an \(\varepsilon\) in \(\O(\log n)\) time. The construction of this datastructure relies on a deep analysis of the decisions of the simplification algorithms. We show how to construct such a datastructure in \(\O(n^4 \log n)\) time and \(\O(n^3)\) space. Interestingly, we can show that this problem is much simpler in the Euclidean case than in the Manhattan or Chebyshev case in multiple aspects unlike the regular simplification algorithms which are distance-agnostic when we accept an equations solving algorithm as a black box.

In \cref{sec:global_imai_iri}, we try to address the problem of finding subcubic algorithms in two dimensions. In the local variants this has been possible by improving the algorith from \citeauthor{computational_geometric_methods_for_polygonal_approximations_of_a_curve}. Using this as motivation, our goal is to adapt the same algorithm to the global variant in order to apply local techniques. The resulting algorithm is functionally equivalent to the algorithm from \citeauthor{global_curve_simplification} but allows a more intuitive understanding of it.

\cref{sec:evaluation} presents vast experimental data on the implemented algorithms which to our knowledge is the first experimental data on the global variants. We measure performance, simplification size, and vary a multitude of parameters and show their effect.

Finally, we conclude with \cref{sec:discussion_conclusion} and mention possible future work.
 
