% some hacky stuff for editor to properly find bibliography
\let\oldbibliography\bibliography % Save the original \bibliography command
\renewcommand{\bibliography}[1]{} % Redefine \bibliography to do nothing
\bibliography{bibliography}
\renewcommand{\bibliography}{\oldbibliography} % Redefine \bibliography to do nothing


\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}

\newcommand{\parenth}[1]{\left(#1\right)}
\newcommand{\angl}[1]{\left\langle#1\right\rangle}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}

\SetKw{Continue}{continue}
\SetKw{Break}{break}

\renewcommand*{\O}{\mathcal{O}}

\section{Introduction}
\label{sec:introduction}

\section{Related Work}
\label{sec:related_work}
This report is mainly based on the previous work from \citeauthor{polyline_simplification_has_cubic_complexity_bringmannetal}
and \citeauthor{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance}, both of which construct algorithms to solve the problem of global polyline simplification under the Fréchet distance.
\citeauthor{polyline_simplification_has_cubic_complexity_bringmannetal}'s algorithm is itself an variant of \citeauthor{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance}'s algorithm that reduces the runtime down to cubic runtime.  
We implement \citeauthor{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance}'s algorithm and explain thoroughly necessary implementation details for every step of the algorithm. We also mention possible optimizations to reach more practical algorithms.
In a coming thesis we will also implement \citeauthor{polyline_simplification_has_cubic_complexity_bringmannetal}'s algorithm and compare the two. 

Most, if not all, previously known algorithms in the general topic of polyline simplification and the Fréchet distance require explicit computation of the roots of polynomials which results in the need for square roots in the Euclidean case and impractical algorithms for general Minkowski distances. We introduce implicit decision problems and show how to reformulate many algorithms using them. We explicitly derive the implicit version of the Fréchet distance decision algorithm from \citeauthor{computing_the_frechet_distance_between_two_polygonal_curves} and the polyline simplification algorithm from \citeauthor{on_optimal_polyline_simplification_using_the_hausdorff_and_frechet_distance}.

\section{Preliminaries}
\label{sec:preliminaries}

In this section we cover notation, conventions, and definitions used throughout this report.  

We denote the natural numbers \(\N\) and they contain \(0\) and the positive natural numbers \(\N_+ = \N \setminus\set{0}\).

\subsection{Polylines}
\label{ssec:polylines}
The main geometric figure we are analysing is the \emph{polyline}.
\begin{definition}[Polyline]
  Let \(d\in \N_+\) and \(n \in \N\) be natural numbers and \(u_0, \dots, u_n \in \R^d\) be \(d\)-dimensional points. 

  The sequence \(P = \angl{u_0, \dots, u_n}\) is a \(d\)-dimensional \emph{polyline} of length \(n\). It consists of \(n+1\) many points which are connected by \(n\) line segments. 

  \begin{itemize}
    \item We interpret \(P\) as a function \(P:[0,n] \to \R^d\) such that \(P(i) = u_i\) for \(i \in \set{0, \dots, n}\).

      Points inbetween are linearly interpolated, meaning that for \(t \in [0, 1]\) and \(i \in \set{0, \dots, n - 1}\) we set \(P(i + t) = (1- t)u_i + t u_{i+1} = u_i + t(u_{i+1} - u_i)\)
    \item We denote \(P[t'\dots t]\) to be the subpolyline from \(t' \in [0, n]\) to \(t \in [t', n]\). Formally, \[P[t'\dots t] = \angl{P(t'), P(\floor{t'} + 1),  P(\floor{t'} + 2) \dots, P(\ceil{t} - 1), P(t)}.\]
  \end{itemize}

\end{definition}
Throughout this report we denote the dimension as \(d\). Polylines are mostly written as capital letters \(P\) and \(Q\) unless they are single line segments, in which case we mostly call them \(e\). The length of a polyline is generally \(n\), \(p\), or \(q\) where \(p\) and \(q\) are generally the lengths of \(P\) and \(Q\) and \(n\) is used when only discussing a single polyline.


\subsection{Distances}
\label{ssec:distances}
We distinguish between distance measurements between two points and distance measurements between two polylines. 
\begin{definition}[Distances]\label{def:point_distance}
  Let \(d \in \N_+\) and \(\ell \geq 1\) be a real number.
  \begin{itemize}
    \item We define the \emph{unnormalized \(\ell\)-Minkowski distance} \(\delta'_\ell\) as 
      \[\delta'_\ell:\R^d \times \R^d \to \R_{\geq 0}, (u, v) \mapsto \sum_{i = 1}^d |u_i - v_i|^\ell.\]
    \item We define the \emph{(normalized) \(\ell\)-Minkowski distance} \(\delta_\ell\) as 
      \[\delta_\ell:\R^d \times \R^d \to \R_{\geq 0}, (u, v) \mapsto \delta'_\ell(u, v)^{\frac1\ell} = \parenth{\sum_{i = 1}^d |u_i - v_i|^\ell}^{\frac1\ell}.\]
    \item In the case of \(\ell = 2\), we call \(\delta_2'\)  the \emph{unnormalized Euclidean distance} and \(\delta_2\) the \emph{(normalized) Euclidean distance}.
    \item In the case of \(\ell = 1\), the unnormalized and normalized versions coincide. We call \(\delta_1' = \delta_1\) the \emph{Manhattan distance}. 
    \item We further define the \emph{Chebyshev distance} \(\delta'_\infty = \delta_\infty\) as 
      \[\delta_\infty:\R^d \times \R^d \to \R_{\geq 0}, (u, v) \mapsto \max_{i = 1, \dots, d} |u_i - v_i|.\]
    \item We define the function \(\nu_\ell:\R_{\geq 0} \to \R_{\geq 0}\) by \(\nu_\ell(x) = x^\ell\) for \(\ell \neq \infty\) and \(\nu_\infty(x) = x\).
  \end{itemize}

  If \(\ell\) is clear from the context or irrelevant, we will omit it.
\end{definition}

The Euclidean distance is the most important case and commonly used distance measure. The Manhattan distance and Chebyshev distance are simple to compute as they do not require exponentiation to rational numbers. When the points have rational coordinates, both distances will produces rational numbers.
The other Minkowski distances are less used and less numerically stable. For this reason we introduce their unnormalized versions and will see how some of the algorithms can be adjusted to use them instead. 

We finally define a distance function on polylines which will be used throughout this report. 

\begin{definition}[Fréchet Distance]
  Let \(d \in \N_+\) and \(\delta\) be a (normalized) distance function. The \emph{Fréchet distance} \(\delta^F\) for two polylines \(P\) and \(Q\) of lengths \(p\) and \(q\) respectively is 
  \[\delta^F(P, Q) = \inf_{f,g} \max_{t \in [0,1]}\delta(P(f(t)), Q(g(t))),\]
  where the infimum is taken over all continuous monotone functions \(f:[0,1] \to [0, p]\) and \(g:[0,1] \to [0,q]\) with \(f(0) = 0\), \(f(1) = p\), \(g(0) = 0\), and \(g(1) = q\).
\end{definition}

With this we can finally define our objective throughout this report. 

\begin{definition}[Polyline Simplification]
  Let \(P\) be a polyline of length \(n\) and \(\varepsilon > 0\) be a fixed real number. Let \(\delta\) be a known distance function. 

  The objective of polyline simplification is to find a minimal subsequence \(S\) of the defining points of \(P\) that contains both the start \(P(0)\) and end \(P(n)\) and \(\delta^F(P, Q) \leq \varepsilon\).
\end{definition}

This is also known as polyline simplification using the \emph{global Fréchet distance} as the distance measurement for the polylines is the Fréchet distance which is applied to the whole resulting polyline and simplification. In contrast, there is also the local version which is more strict as for each line segment \(e = \overline{S(i)S(i+1)}\) where \(S(i) = P(j')\) and \(S(i+1) = P(j)\) must satisfy that \(\delta^F(e, P[j' \dots j]) \leq \varepsilon\). Further, there are also both the local and global \emph{Hausdorff distance} as measurements. Here, we only consider the global Fréchet distance.

\subsection{Properties of Distances}
Here, we want to briefly cover some properties of the just defined distance functions, both for points and polylines.

All distance functions we have introduced are examples of metrics on \(\R^d\) (see, e.g., \cite{metric_spaces} for respective proofs).

\begin{definition}[Metric Spaces]\label{def:metric}
  Let \(X\) be a set and \(\delta:X\times X \to \R\). Then \(\delta\) is a \emph{metric} on \(X\) if, and only if, for each \(a, b, c \in X\), 
  \begin{itemize}
    \item \(\delta(a, b) \geq 0\) with equality if, and only if, \(a = b\), \hfill (Positivity)
    \item \(\delta(a, b) = \delta(b, a)\), and \hfill (Symmetry)
    \item \(\delta(a, c) \leq \delta(a, b) + \delta(b, c)\). \hfill (Triangle Inequality)
  \end{itemize}

  A set \(X\) together with a metric \(\delta\) is called a \emph{metric space}.
\end{definition}

We also note an obvious, but useful fact which allows avoiding roots when comparing distances. 
\begin{observation}\label{obs:unnormalize}
  Let \(\ell \in [1, \infty]\), \(\varepsilon > 0\), and \(u, v \in \R^d\). Then 
    \[\delta_\ell(u, v) \leq \varepsilon \iff \delta_\ell'(u, v) \leq \nu_\ell(\varepsilon).\]
\end{observation}


\begin{lemma}\label{lem:distance_properties}
  Let \(\delta\) be any Minkowski distance (including the Chebyshev distance).
  \begin{enumerate}
    \item \(\delta(u, v) = \delta(u - w, v - w)\) for all \(u, v, w\in \R^d\).
    \item \(\delta(a u, a v) = |a| \delta(u, v)\) for all \(u, v\in \R^d\)  and \(a \in \R\).
    \item If \(\delta(u, w) \leq \varepsilon\) and \(\delta(v, w) \leq \varepsilon\) then also \(\delta(u + t(v-u), w) = \delta((1-t)u + tv, w) \leq \varepsilon\) for all \(u, v, w\in \R^d\), \(\varepsilon > 0\) and all \(t \in [0, 1]\).
    \item \(\delta^F(\overline{ab}, \overline{cd}) \leq \varepsilon\) if, and only if, \(\delta(a, c) \leq \varepsilon\) and \(\delta(b, d) \leq \varepsilon\) for \(a,b,c,d \in \R^d\).
  \end{enumerate}
\end{lemma}

The third property implies that \(\set{u \mid \delta(u, v) \leq \varepsilon }\) is a convex set for fixed \(v \in \R^d\) and \(\varepsilon >0 \).
The fourth property gives us a characterization of the Fréchet distance when comparing single line segments. 

\begin{proof}
  \begin{enumerate}
    \item Follows directly from the definitions 
    \item Follows directly from the definitions 
    \item \begin{flalign*}
        \delta((1-t)u + tv, w) &\overset{(1)}= \delta((1-t)u + tv- w, 0)\\
         &= \delta((1-t)(u-w)+t(v-w), 0) \\
         &= \delta((1-t)(u-w),t(w-v)) \\
         &\leq \delta((1-t)(u-w),0) + \delta(0,t(w-v)) && \textrm{Triangle Inequality}\\
         &\overset{(2)}= (1-t)\delta(u,w) + t\delta(v,w) \leq (1-t)\varepsilon + t\varepsilon = \varepsilon\\
    \end{flalign*}
  \item \(\Rightarrow\) follows as the first and last points of the two polylines must match (the minimization in the definition of the Fréchet distance is over functions \(f, g\) with \(f(0) = g(0) = 0\) and \(f(1) = g(1) = 1\)).
    Let For \(\Leftarrow\) we construct respective functions \(f\) and \(g\), in fact, the identity functions suffices for both, i.e., \(f(t) = g(t) = t\) suffices as 
    \begin{flalign*}
      \delta((1-f(t))a + f(t)b, (1-g(t))c + g(t)d) &= \delta((1-t)a + tb, (1-t)c + td) \\
                                                   &\overset{(1)}= \delta((1-t)(a-c), t(d-b)) \\
                                                   &\leq \delta((1-t)(a-c), 0) + \delta(0, t(d-b)) && \textrm{Triangle Inequality}\\
                                                   &\overset{(2)}= (1-t)\delta(a, c) + t\delta(b, d) \leq \varepsilon \\
    \end{flalign*}
  \end{enumerate}
\end{proof}

% section Algorithm and Implementation
\input{section_algorithm_implementation.tex}

% section Implicit Polyline Simplification
\input{section_implicit_algorithm.tex}

\section{Experimental Evaluation}
\label{sec:evaluation}
% test if sqrt less version is better for euclidean or not, probably not - actually yes 
%
%
\subsection{Experimental Setup}
\label{subsec:exp_setup}

\subsection{Data and Hardware}
\label{subsec:hardware}

\subsubsection{Software and Data}
\label{subsubsec:software}
The algorithms were implemented in C++ and compiled using GCC with all optimizations. For parallelization we used OpenMP using dynamic scheduling with a chunk size of \(32\) to run the loops over \(i\) and \(j\) in parallel. 

The polylines which were used as test cases were automatically generated using multiple parameters. These include the minimum and maximum length of all line segment as well as the maximum angle that two consecutive line segments could deviate by, i.e., for a maximum angle of \(0\)° the whole polyline would be a straight line while a maximum angle of \(180\)° allows any direction. The reasoning behind this is that a small angle forces the polyline to be more smooth while a large angle allows erratic polylines. We test both smooth and erratic ones, the parameters are given for each test case. 

For the data generation we fix the first point of the polyline to be the origin and sample an initial angle arbitrarily uniformly distributed. Following that we iteratively sample a line segment length and an angle which is added to the current total angle and using these value the next point is generated. The angle is sampled uniformly in the range \([-\alpha, \alpha]\) where \(\alpha\) is the maximum angle. For the line segment length we use inverse transform sampling as a uniform distribution skews the lower line segment lengths closer together because angle differences are not reflected the same way for small and large line segment lengths. Thus we use \(\sqrt[d]{m^d + (M^d - m^d) U}\) where \(d\) is the dimension, \(m\) is the minimum line length, \(M\) is the maximum line length, and \(U\) is a uniform distribution on \([0,1]\).

Theoretically, our data generator works in arbitrary dimension but we mainly test two-dimensional ones as a particularly useful case that can be visualized well. 

\subsubsection{Hardware}
\label{subsubsec:hardware}
The code was tested on a laptop with an AMD Ryzen 5 7520U, 4.38 GHz CPU on eight cores with the Arch Linux operating system.


\subsection{Results}
\label{subsec:results}
Before we show the results, we note that even minor changes in the implementation such as inlining a function, changing the order of comparisons and other details affect the practical runtime heavily, especially for the implicit approach. We suspect that this is because of the technical simplicity of the algorithm in that it is five nested loops (or six including the first reachable point subroutine). Thus changes in the innermost loops propagate heavily. We cannot mention all these low-level decisions without explaining in depth our implementation which is outside the scope of this report. We chose the versions that generally gave us the best performance for well-behaved polylines on two dimensions as that is the most interesting case for us. For others that want to implement and test these algorithms, we advise testing multiple equivalent approaches as simple design choices can affect the practical runtim by upto 20\% in either direction. 

The choice of using a dynamic instead of a static scheduler for parallelization and the chunk size of \(32\) were also determined by testing. We suspect that a static scheduler fails to fully utilize the cores as it would subdivide the looped ranges before running the loop which causes less utilization for unbalanced iterations. However, we generally do expect unbalanced iterations when using optimizations as not all iterations allow the same optimizations. Furthermore, those iterations that require more work typically are close together in the sense of close \(i\) and \(j\) values. This is because the results of the optimizations are not independent even though the computation is: When allowing a sightly different \(i\) or \(j\) the result may not change drastically and thus the computational load stays comparable. This is bad for a static scheduler as most cores would be done rather fast with only few doing most of the work. 

The results we show are the effect of data, i.e., well-behaved vs. non-well-behaved polylines, the effects of parallelization, some combinations of the optimizations we outlined in \cref{ssec:optimizations}, as well as a comparison of the explicit and implicit implementations. 



% compare 
% - explicit v implicit
% - parallel v non parallel 
% - optimized v non optimized (multiple optimizations, turn off individually)
% - ugly v nice 

% notes 
% - minor changes can have big impact (eg inlining)

\section{Conclusions and Future Work}
\label{sec:discussion_conclusion}

% Questions: 
% - Can implicit Manhattan and Chebyshev be implemented in linear time? without division? 
% - Can general implicit Minkowski be solved without approximations? Maybe using some techniques from real algebraic geometry
% - Can the runtime of Van Kreveld et al algorithm be analyzed such that it has a better runtime with the optimizations? Maybe for a subset of polylines only 
% - Can space consumption reach quadratic? Maybe by using tree trimming 
% - 



